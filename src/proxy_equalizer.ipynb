{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy Equalizer -- Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "### General Todos\n",
    "\n",
    "* Go through everything and make sure order of variables in graphs/samples/stacking/networks etc. is maintained/fixed somehow and not scrambled up by set/dictionary conversions at some point.\n",
    "* Freezing the layers in `interventions._copy_and_freeze()` is the last hard coded part. Make it flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from mlp import MLP, train\n",
    "from sem import SEM\n",
    "from interventions import Interventions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the graph for the SEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to set up a structural equation model.\n",
    "It consists of a graph and the corresponding equations.\n",
    "We initialize an `SEM` object by passing in a graph as a dictionary. (Details of the data structure are in the docstring of the `SEM` class.\n",
    "\n",
    "We can then draw the graph with `sem.draw()` and print a lot of information about it with `sem.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem = SEM({\"Np\": None, \"A\": None, \"Nx\": None, \"P\": [\"Np\", \"A\"], \"X\": [\"A\", \"P\", \"Nx\"], \"Y\": [\"P\", \"X\"]})\n",
    "sem.summary()\n",
    "sem.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the structural equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first check the status of the vertices to make sure we attach valid equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All vertices\n",
    "print(\"All vertices: \", sem.vertices())\n",
    "# Root vertices => provide distributions\n",
    "print(\"Roots: \", sem.roots())\n",
    "# Non root vertices => provide equations making use of all parents\n",
    "print(\"Non-roots: \", sem.non_roots())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we attach structural equations to the vertices with `sem.attach_equation(vertex, callable)`.\n",
    "For the root vertices, we draw from a standard normal.\n",
    "\n",
    "The only argument to the callable is an integer `n`, the number of samples to draw. Of course, we could also attach different distributions separately.\n",
    "\n",
    "**Note**: The `callable` attached to a vertex needs to return a `torch.tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in sem.roots():\n",
    "    sem.attach_equation(v, lambda n: torch.randn(n, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the non-root vertices we attach made up functions.\n",
    "\n",
    "The only argument to the callable for non-roots is a dictionary `data` that must have the vertex names as keys. This example shows how the parent vertices are accessed. We just construct a fully linear model in which all coefficients are just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem.attach_equation(\"P\", lambda data: 1 * data['Np'] + 1 * data['A'])\n",
    "sem.attach_equation(\"X\", lambda data: 1 * data['A'] + 1 * data['P'] + 1 * data['Nx'])\n",
    "sem.attach_equation(\"Y\", lambda data: 1 * data['P'] + 1 * data['X'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the SEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the SEM is fully specified and we can draw samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sample = sem.sample(8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `utils` module contains functions for plotting whole samples, where each variable is plotted as a function of its parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_samples(sem, orig_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn the structural equations from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in this example we provided analytical equations for the structural equation model, in reality we only get data. Our assumptions are that we guessed the causal graph correctly, but we do not know the structural equations. We assume that we have a observed samples from the graph. In this example, we will use the generated sample as our observed data.\n",
    "\n",
    "Given the graph and the observed data, we can now try to learn the structural equations. **Note**: This can be done even if we had not attached structural equations to the `SEM` object.\n",
    "\n",
    "**Arguments**: We pass in our \"observed\" sample, and can specify the number and sizes of hidden layers by `hidden_sizes` (default: `()` i.e. no hidden layers). Moreover, we can pass a list of vertices to the `binarize` keyword to add a `torch.nn.Sigmoid()` layer at the end when predicting those vertices (default: `[]`). Further, we can pass `epochs` (default: `50`) and `batchsize` (default: `32`) as named arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem.learn_from_sample(sample=orig_sample, hidden_sizes=(), binarize=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at what networks have been learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem.learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For smaller networks (especially in the linear case with no hidden layers), it can be insightful to check whether the learned parameters match the actual coefficients in the analytical equations from which the sample was generated. In our simple case we get only ones, so we almost perfectly learned the linear equations (unsurprisingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem.print_learned_parameters(weights=True, biases=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the learned equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to how we sampled from the analytical structural equations before, we can now sample from the learned equations.\n",
    "\n",
    "Note, however, that we did not learn the distributions for the root vertices. Hence we have to provide values for the root vertices and can then pass those down to predict the other vertices with our learned functions with the `predict_from_sample()` function. Without further arguments, it does not mutate the input, but returns a new sample that has identical values for the root vertices and updates all non-root vertices with predictions from the learned functions.\n",
    "\n",
    "**Note**: The `predict_from_sample()` function is more flexible. One can choose manually which vertices to update (`update` argument), whether to mutate the passed in sample instead of creating a new one with `mutate=True` (then the return value is `None`) and also to use a different predictor for specified vertices by `replace={vertex: predictor}`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_sample = sem.predict_from_sample(orig_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the original sample and the learned sample simultaneously by passing a list of samples to `utils.plot_samples()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_samples(sem, [orig_sample, learned_sample], legend=['analytic', 'learned'], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fully linear case, we recover the original sample basically perfectly, i.e. we learned the structural equations exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our self made format to specify interventions. In a dict, for each proxy variable, we store another dict, which we call `functions`. In `functions`, keys are preset strings that correspond to the `known_functions` in the `Intervention` class. Current options: `'randn'`, `'rand'`, `'const'`, `'range'`. Every value of `functions` must be a list of tuples (!), where the tuples hold one or multiple scalar arguments (depending on the key).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "This specifies five different intervened values for the proxy `'P1'` and four different intervened values for the proxy `'P2'`, a total of `5 * 4 = 20` different intervened samples.\n",
    "\n",
    "```python\n",
    "intervention_spec = {\n",
    "    'P1': {\n",
    "          'randn': [(0, 3), (0, 3), (0, 5)],\n",
    "          'const':[(1,), (0,)],\n",
    "          },\n",
    "    'P2': {\n",
    "          'range': [(-1, 1), (-5, 5)],\n",
    "          'rand':[(-1, 1), (-5, 5)],\n",
    "          },\n",
    "    }\n",
    "```\n",
    "\n",
    "Note that `Interventions` also takes a sample as an argument. Currently, interventions are done on an existing sample, i.e. first, we compute the intervened graph, given the proxies specified in the `intervention_spec`. Then we copy the sample `n_interventions` times and fill the proxy values in each sample with one of the possible combination of specified interventions. In the intervened graph, we then update all descendents of the proxies (in topological order), where we might also need values from other root vertices. This is why we already provide a sample.\n",
    "\n",
    "Strictly, this corresponds to neither counterfactuals nor interventions. As always there's no \"right\" way to this, but I'm happy for your opinions on the following options:\n",
    "\n",
    "1. Always use one single sample for the other root vertices in the intervened graph:\n",
    "    a. Use the same original sample that was used to learn the equations.\n",
    "    b. Draw a new \"base sample\" for the retraining part.\n",
    "2. For each intervened sample, draw the other root vertices in the intervened graph anew.\n",
    "\n",
    "Consider also:\n",
    "\n",
    "* In reality, we do not observe a full sample of the graph (root vertices are not observed).\n",
    "* Can we make assumptions about distribution of root vertices in real life, e.g. Gaussian? If so, how do we find the corresponding root vertex values belonging to one specific observation. (If we see P, X, Y, how do we find the corresponding Nx, A, Np?) While the distributions are enough to sample new values, the specific corresponding values are needed to learn the equations in the first step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear example, we choose random normal distributions with different variances as interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_spec = {\n",
    "    'P': {\n",
    "         'randn': [(0, 3), (0, 3), (0, 5), (0, 5)],\n",
    "         },\n",
    "    }\n",
    "interventions = Interventions(sem, orig_sample, intervention_spec)\n",
    "interventions.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a corrected version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually we can actually retrain part of the target network, in this case the network for `'Y'` to minimize the variance of predictions across all different intervened samples. Note that here it seems like it only makes sense to do this for the same values of root vertices (closer to counterfactual?), because why would I want similar `'Y'` values for completely different starting values? On the other hand, we want that to be true in distribution, hence for a large batch size, we could also try to enforce that criterion with different values for the root vertices in each intervened sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = interventions.train_corrected(epochs=100, batchsize=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the corrected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small linear models: check parameters directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this small linear network we can look directly at the parameters it has learned. We indeed see that it learns the ones everywhere originally and in the corrected version has a -1 for `'P'` instead, exactly what theory demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print(\"Original weights:\")\n",
    "sem.print_learned_parameters(show=['Y'], weights=True, biases=False)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Fair parameters:\")\n",
    "for name, param in corrected.named_parameters():\n",
    "    if 'bias' not in name:\n",
    "        print(param.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison on a new sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the full sample plots we have already encountered above for a new sample, its learned reproduction and the corrected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base, orig, fair = utils.evaluate_on_new_sample(sem, 'Y', corrected, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already seen, the learned perfectly recovers the original sample from the analytical structural equation model. The fair results coincide up to the target value `'Y'` of course, because we did not touch any other part. The dependence of `'Y'` on both `'P'` and `'X'` has been decreased, but is **not** zero (see next section for an explanation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation tools for linear prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear case, we can also look at (print and plot) all sorts of correlations, i.e. the slopes, r-values (Pearson Correlation Coefficient), p-values and standard errors of these tests.\n",
    "\n",
    "We see that the correlation between `'Yfair'` and `'P'` goes down as compared to `'Y'` and `'P'`, but is **not** zero. There is still correlation bettwen `'Yfair'` and `'P'` left through the confounder `'A'`. This is the main difference to all \"learning fair representation\" approaches so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_correlations(orig, sem=sem, sources=['A', 'P', 'X'], targets=['Y', 'Yfair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = sem.vertices() + ['Yfair']\n",
    "utils.plot_correlations(orig, sem=sem, sources=all_vars, targets=all_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick run through a binarized example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go through the whole workflow from specifying a graph to the final evaluation (without unnecessary intermediate steps), where we binarize the value of `'P'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the graph\n",
    "sem = SEM({\"Np\": None, \"A\": None, \"Nx\": None, \"P\": [\"Np\", \"A\"], \"X\": [\"A\", \"P\", \"Nx\"], \"Y\": [\"P\", \"X\"]})\n",
    "\n",
    "# Attach equations\n",
    "for v in sem.roots():\n",
    "    sem.attach_equation(v, lambda n: torch.randn(n, 1))\n",
    "sem.attach_equation(\"P\", lambda data: (1 * data['Np'] + 1 * data['A'] > 0.0).float())\n",
    "sem.attach_equation(\"X\", lambda data: 1 * data['A'] + 5 * data['P'] + 1 * data['Nx'])\n",
    "sem.attach_equation(\"Y\", lambda data: 1 * data['P'] + 1 * data['X'])\n",
    "\n",
    "# Learn the equations (internally computes sample), use hidden layer for demo purposes\n",
    "orig = sem.learn_from_sample(hidden_sizes=(128,), epochs=50, binarize=['P'])\n",
    "learned_sample = sem.predict_from_sample(orig_sample)\n",
    "\n",
    "# Specify interventions, this time constants 0 and 1\n",
    "# intervention_spec = {'P': {'const': [(0,), (1,)], 'range': [(0, 1)], 'rand': [(0, 1)]}}\n",
    "intervention_spec = {'P': {'const': [(0,), (1,), (0,), (1,), (0,), (1,), (0,), (1,)]}}\n",
    "interventions = Interventions(sem, orig_sample, intervention_spec)\n",
    "\n",
    "# Remove proxy discrimination\n",
    "corrected = interventions.train_corrected(epochs=100, batchsize=64)\n",
    "                    \n",
    "# Evaluate on new sample\n",
    "base, orig, fair = utils.evaluate_on_new_sample(sem, 'Y', corrected, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation tools for binary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = copy.deepcopy(orig_sample)\n",
    "s2 = copy.deepcopy(fair_sample)\n",
    "s1['Y'] = (s1['Y'] > 0.5).float()\n",
    "s2['Y'] = (s2['Y'] > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_samples(sem, [test_sample, s1, s2], legend=['analytical', 'learned', 'fair'], alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(s1['Y'].int().numpy(), s2['Y'].int().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEVELOPMENTAL STAGE -- DEPRECATED BEYOND THIS POINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import tqdm\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from mlp import MLP, train\n",
    "from graph import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal multilayer perceptron + training (now in `mlp.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple fully connected feed forward network.\"\"\"\n",
    "    def __init__(self, sizes, final=None):\n",
    "        \"\"\"\n",
    "        Initialize the network.\n",
    "        \n",
    "        A variable size network with only fully connected layers and ELU activations after all but the last layer.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        sizes: A list of the numbers of neurons in the layers.\n",
    "               len(sizes)-1 is the number of layers.\n",
    "               First and last entries are input and output dimension.\n",
    "        final: What to use as a final layer, e.g. torch.nn.Sigmoid()\n",
    "               None (default) means no final layer (regression vs. classification).\n",
    "               \n",
    "        Example:\n",
    "            A network with 2-dimensional input, one hidden layer with 128 neurons and 1-dimensional output for regression.\n",
    "            >>> net = MLP([2, 128, 1])\n",
    "            \n",
    "            A network with 10-dimensional input, two hidden layers of 128 and 256 neurons and 1-dimensional output for classification.\n",
    "            >>> net = MLP([10, 128, 256, 1], final=torch.nn.Sigmoid())            \n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        # If there is only one input dimension, everything is fine\n",
    "        if sizes[0] == 1:\n",
    "            self.layers.append(nn.Linear(sizes[0], sizes[1]))\n",
    "        # For multiple input dimensions, each one has a separate following hidden layer.\n",
    "        # This is necessary for the partial training later on.\n",
    "        else:\n",
    "            self.layers.append(nn.ModuleList([nn.Linear(1, sizes[1]) for _ in range(sizes[0])]))\n",
    "            \n",
    "        # Add the remaining layers with elu activations\n",
    "        for i in range(len(sizes) - 1)[1:]:\n",
    "            if i != (len(sizes) - 1):\n",
    "                self.layers.append(nn.ELU()) \n",
    "            self.layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            \n",
    "        if final is not None:\n",
    "            self.layers.append(final)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward pass.\"\"\"\n",
    "        # If there are multiple inputs, add up their hidden layers\n",
    "        if isinstance(self.layers[0], collections.Iterable):\n",
    "            y = self.layers[0][0](x[:, 0, None])\n",
    "            for i in range(1, len(self.layers[0])):\n",
    "                y += self.layers[0][i](x[:, i, None])\n",
    "            return nn.Sequential(*[self.layers[i] for i in range(1, len(self.layers))])(y)\n",
    "        # Otherwise just build a simple sequential model\n",
    "        else:\n",
    "            return nn.Sequential(*self.layers)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, x, y, loss_func=nn.MSELoss(), epochs=50, batchsize=32):\n",
    "    \"\"\"\n",
    "    Train a network.\n",
    "    \n",
    "    Args:\n",
    "        net:       A network module.\n",
    "        x:         Training input data.\n",
    "        y:         Training labels.\n",
    "        loos_func: Loss function, default is nn.MSELoss(), i.e. mean squared error.\n",
    "        n_epochs:  Number of training epochs.\n",
    "    \"\"\"\n",
    "    opt = torch.optim.Adam(net.parameters())\n",
    "    n_samples = x.size(0)\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data\n",
    "        p = torch.randperm(n_samples).long()\n",
    "        xp = x[p]\n",
    "        yp = y[p]\n",
    "\n",
    "        for i1 in range(0, n_samples, batchsize):\n",
    "            # Extract a batch\n",
    "            i2 = min(i1 + batchsize, n_samples)\n",
    "            xi, yi = xp[i1:i2], yp[i1:i2]\n",
    "\n",
    "            # Reset gradients\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = loss_func(net(Variable(xi)), Variable(yi))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Parameter update\n",
    "            opt.step()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph representation (now in `graph.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \"\"\"A light weight, self made graph representation.\"\"\"\n",
    "    def __init__(self, graph):\n",
    "        \"\"\"Initialize a Graph object.\"\"\"\n",
    "        if isinstance(graph, dict):\n",
    "            self.graph = graph\n",
    "        else:\n",
    "            print(\"Could not process input {} as graph. Initialized empty graph.\".format(graph))\n",
    "            self.graph = None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"Define representation.\"\"\"\n",
    "        import pprint\n",
    "        return pprint.pformat(self.graph)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Define string format.\"\"\"\n",
    "        import pprint\n",
    "        return pprint.pformat(self.graph)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.graph)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.graph[item]\n",
    "    \n",
    "    def _try_add_vertex(self, vertex):\n",
    "        if vertex in self.graph:\n",
    "            print(\"Vertex already exists.\")\n",
    "        else:\n",
    "            self.graph[vertex] = None\n",
    "            print(\"Added vertex \", vertex)\n",
    "    \n",
    "    def _try_add_edge(self, source, target):\n",
    "        if source in self.graph:\n",
    "            if target not in self.graph[source]:\n",
    "                self.graph[source].append(target)\n",
    "            else:\n",
    "                print(\"Edge already exists.\")\n",
    "        else:\n",
    "            self.graph[source] = [target]\n",
    "    \n",
    "    def add_vertices(self, vertices):\n",
    "        \"\"\"Add one or multiple vertices to the graph.\"\"\"\n",
    "        if isinstance(vertices, collections.Iterable):\n",
    "            for v in vertices:\n",
    "                _try_add_vertex(v)\n",
    "        else:\n",
    "            _try_add_vertex(v)\n",
    "\n",
    "    def add_edge(self, source, target):\n",
    "        \"\"\"Add a single edge from source to target.\"\"\"\n",
    "        self._try_add_edge(source, target)\n",
    "        \n",
    "    def vertices(self):\n",
    "        \"\"\"Find all vertices.\"\"\"\n",
    "        return list(self.graph.keys())\n",
    "    \n",
    "    def edges(self):\n",
    "        \"\"\"Find all edges.\"\"\"\n",
    "        edges = []\n",
    "        for node, parents in self.graph.items():\n",
    "            if parents is not None:\n",
    "                for p in parents:\n",
    "                    edges.append({p: node})\n",
    "        return edges\n",
    "\n",
    "    def roots(self):\n",
    "        \"\"\"Find all root vertices.\"\"\"\n",
    "        return [node for node in self.graph if self.graph[node] is None]\n",
    "\n",
    "    def non_roots(self):\n",
    "        return [node for node in self.graph if self.graph[node] is not None]\n",
    "    \n",
    "    def leafs(self):\n",
    "        \"\"\"Find all leaf vertices.\"\"\"\n",
    "        return list(set(self.vertices()).difference(self.non_leafs()))\n",
    "\n",
    "    def non_leafs(self):\n",
    "        \"\"\"Find all non-leaf vertices.\"\"\"\n",
    "        return list(set(sum([p for p in self.graph.values() if p is not None], [])))\n",
    "    \n",
    "    def parents(self, vertex):\n",
    "        \"\"\"Find the parents of a vertex.\"\"\"\n",
    "        return self.graph[vertex]\n",
    "\n",
    "    def children(self, vertex):\n",
    "        \"\"\"Find the children of a vertex.\"\"\"\n",
    "        children = []\n",
    "        for node, parents in self.graph.items():\n",
    "            if parents is not None and vertex in parents:\n",
    "                children.append(node)\n",
    "        return children\n",
    "    \n",
    "    def descendents(self, vertex):\n",
    "        \"\"\"Find all descendents of a vertex.\"\"\"\n",
    "        descendents = []\n",
    "        current_children = self.children(vertex)\n",
    "        if not current_children:\n",
    "            return descendents\n",
    "    \n",
    "        descendents += current_children\n",
    "    \n",
    "        for child in current_children:\n",
    "            new_descendents = self.descendents(child)\n",
    "            descendents += new_descendents\n",
    "\n",
    "        return list(set(descendents))\n",
    "    \n",
    "    def get_intervened_graph(self, interventions):\n",
    "        \"\"\"Return the intervened graph as a new graph.\"\"\"\n",
    "        intervened_graph = copy.deepcopy(self.graph)\n",
    "        if isinstance(interventions, collections.Iterable):\n",
    "            for i in interventions:\n",
    "                intervened_graph[i] = None\n",
    "        else:\n",
    "            intervened_graph[interventions] = None\n",
    "        return Graph(intervened_graph)\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print summary of the graph.\"\"\"\n",
    "        print(\"Vertices in graph\", self.vertices())\n",
    "        print(\"Roots in graph\", self.roots())\n",
    "        print(\"Non-roots in graph\", self.non_roots())\n",
    "        print(\"Leafs in graph\", self.leafs())\n",
    "        print(\"Non-leafs in graph\", self.non_leafs())\n",
    "        print(\"Edges in the graph\", self.edges())\n",
    "\n",
    "        for v in self.vertices():\n",
    "            print(\"Children of {} are {}\".format(v, self.children(v)))\n",
    "            print(\"Parents of {} are {}\".format(v, self.parents(v)))\n",
    "            print(\"Descendents of {} are {}\".format(v, self.descendents(v)))\n",
    "        \n",
    "    def _convert_to_nx(self):\n",
    "        import networkx as nx\n",
    "        G = nx.DiGraph()\n",
    "        for edge in self.edges():\n",
    "            edge = next(iter(edge.items()))\n",
    "            G.add_edge(*edge)\n",
    "        return G\n",
    "\n",
    "    def topological_sort(self):\n",
    "        import networkx as nx\n",
    "        G = self._convert_to_nx()\n",
    "        return list(nx.topological_sort(G))\n",
    "    \n",
    "    def draw(self):\n",
    "        import networkx as nx\n",
    "#         from nxpd import draw, nxpdParams\n",
    "#         nxpdParams['show'] = 'ipynb'\n",
    "        G = self._convert_to_nx()\n",
    "        G.graph['dpi'] = 150\n",
    "        draw(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling (now merged with graph in graph superclass `sem.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we define a causal graph\n",
    "# Nodes are the keys of the graph and the values are the parents(!) of the key.\n",
    "# Setting the value to None means that the node is a root of the graph.\n",
    "graph = Graph({\"Np\": None, \"A\": None, \"Nx\": None, \"P\": [\"Np\", \"A\"], \"X\": [\"A\", \"P\", \"Nx\"], \"Y\": [\"P\", \"X\"]})\n",
    "graph.summary()\n",
    "graph.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default number of examples in a sample\n",
    "n_sample = 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremely hard coded sampling from one given graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(n, eps=0.05):\n",
    "    \"\"\"Generate sample data specific to a graph (hand tuned).\"\"\"\n",
    "    # Randomly sample the root nodes variables\n",
    "    Np = torch.randn(n, 1)\n",
    "    Nx = torch.randn(n, 1)\n",
    "#     A = torch.zeros(n, 1)\n",
    "#     A[torch.randperm(n).long()[:int(n/2)]] = 1.\n",
    "    A = torch.randn(n, 1)\n",
    "    \n",
    "    P = 1 * Np + 3 * A + eps * torch.randn(n, 1)\n",
    "#     P = (1 * Np + 3 * A + eps * torch.randn(n, 1) > 0.0).float()\n",
    "    X = 2 * A + 1 * P + 3 * Nx + eps * torch.randn(n, 1)\n",
    "#     Y = 1 * P + 3 * X + eps * torch.randn(n, 1)\n",
    "    Y = (1 * P + 3 * X + eps * torch.randn(n, 1) > 0.0).float()\n",
    "    return dict(Np=Np, Nx=Nx, A=A, P=P, X=X, Y=Y)\n",
    "\n",
    "def plot_samples(graph, samples):\n",
    "    \"\"\"Plot all relevant dependencies in a graph from a/multiple sample(s).\"\"\"        \n",
    "    # If we did not already receive a list of samples, make one element list\n",
    "    if not isinstance(samples, list):\n",
    "        samples = [samples]\n",
    "    # Get non root variables\n",
    "    non_roots = graph.non_roots()\n",
    "    # Get maximum number of input variables\n",
    "    max_deps = max([len(graph.parents(var)) for var in non_roots])\n",
    "    \n",
    "    fig, axs = plt.subplots(len(non_roots), max_deps, figsize=(5 * max_deps, 5 * len(non_roots)))\n",
    "\n",
    "    # Go through all dependencies and plot them as 2D scatter plots\n",
    "    for i, y_var in enumerate(non_roots):\n",
    "        for j, x_var in enumerate(graph.parents(y_var)):\n",
    "            for sample in samples:\n",
    "                axs[i, j].plot(sample[x_var].numpy(), sample[y_var].numpy(), '.')\n",
    "                axs[i, j].set_xlabel(x_var)\n",
    "                axs[i, j].set_ylabel(y_var)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Pure util function\n",
    "def combine_variables(variables, sample):\n",
    "    \"\"\"Stack variables from sample along new axis.\"\"\"\n",
    "    data = torch.stack([sample[i] for i in variables], dim=1).squeeze()\n",
    "    if len(data.size()) == 1:\n",
    "        data.unsqueeze_(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample should probably come as a pandas dataframe?\n",
    "# But then the things are not torch arrays, so maybe keeping it as a dict is smarter?\n",
    "sample = get_sample(n_sample, eps=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn the _real_ SEM (now also part of `sem.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_sem(graph, sample, hidden_sizes=(), binarize=None):\n",
    "    \"\"\"Given a graph and a sample from it, learn the structural equations.\"\"\"\n",
    "    learned = {}\n",
    "    for vertex in graph.non_roots():\n",
    "        print(\"Training {} -> {}...\".format(graph.parents(vertex), vertex), end=' ')\n",
    "        data = combine_variables(graph.parents(vertex), sample)\n",
    "        if vertex in binarize:\n",
    "            final = nn.Sigmoid()\n",
    "        else:\n",
    "            final = None\n",
    "        learned[vertex] = train(MLP([data.size(-1), *hidden_sizes, 1], final=final), data, sample[vertex])\n",
    "        print(\"DONE\")\n",
    "    return learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(graph, sample, learned):\n",
    "    new_sample = copy.deepcopy(sample)\n",
    "    need_update = [v for v in graph.topological_sort() if v not in graph.roots()]\n",
    "    print(\"Updating the nodes {}...\".format(need_update), end=' ')\n",
    "    for update in need_update:\n",
    "        argument = Variable(combine_variables(graph.parents(update), new_sample))\n",
    "        new_sample[update] = learned[update](argument).data\n",
    "    print(\"DONE\")\n",
    "    return new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_from_sample(self, sample, learned):\n",
    "        from torch.autograd import Variable\n",
    "        new_sample = copy.deepcopy(sample)\n",
    "        need_update = [v for v in self.topological_sort()\n",
    "                       if v not in self.roots()]\n",
    "        print(\"Updating the nodes {}.\".format(need_update))\n",
    "        for update in need_update:\n",
    "            print(\"Updating node {}...\".format(update), end=' ')\n",
    "            argument = Variable(utils.combine_variables(self.parents(update),\n",
    "                                                        new_sample))\n",
    "            new_sample[update] = learned[update](argument).data\n",
    "        print(\"DONE\")\n",
    "        return new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned = learn_sem(graph, sample, binarize='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sample = predict_sample(graph, sample, learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(graph, [sample, pred_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interventions and intervened data sets (now part of `interventions.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self made structure to specify interventions\n",
    "class Interventions:\n",
    "    \"\"\"Manage and create training data sets for interventions.\"\"\"\n",
    "    \n",
    "    # Methods for creating intervened samples\n",
    "    known_functions = {\n",
    "        'randn': (lambda self, mean, var: torch.randn(self.n_samples, 1) * var + mean),\n",
    "        'const': (lambda self, const: torch.ones(self.n_samples, 1) * const),\n",
    "        'rand': (lambda self, start, end: torch.rand(self.n_samples, 1) * (start - end) + end),\n",
    "        'range': (lambda self, start, end: torch.linspace(start, end, steps=self.n_samples).unsqueeze_(1))\n",
    "    }\n",
    "\n",
    "    def __init__(self, graph, base_sample, intervention_spec, target='Y'):\n",
    "        \"\"\"Initialize with a base sample and intervention specification.\"\"\"\n",
    "        self.base_sample = base_sample\n",
    "        self.n_samples = len(next(iter(base_sample.values())))\n",
    "        self.interventions = intervention_spec\n",
    "        self.proxies = list(intervention_spec.keys())\n",
    "        self.graph = graph\n",
    "        self.intervened_graph = self.graph.get_intervened_graph(self.proxies)\n",
    "        self.target = target\n",
    "        self._set_n_interventions()\n",
    "        self.training_samples = []\n",
    "        self._check_input()\n",
    "\n",
    "    def _check_input(self):\n",
    "        \"\"\"Some basic checks of the input.\"\"\"\n",
    "        assert self.target in self.graph.leafs(), \"Can't correct for non-leaf {}\".format(self.target)\n",
    "\n",
    "        for proxy in self.proxies:\n",
    "            assert self.target in self.graph.descendents(proxy), \"Can't correct for non-descendent {} of proxy {}.\".format(self.target, proxy)\n",
    "\n",
    "    def _set_n_interventions(self):\n",
    "        \"\"\"Compute and set the total number of interventions, i.e. training sets.\"\"\"\n",
    "        self.n_interventions = 1\n",
    "        for proxy, funcs in self.interventions.items():\n",
    "            for params in funcs.values():\n",
    "                if not isinstance(params, list):\n",
    "                    params = [params]\n",
    "                self.n_interventions *= len(params)\n",
    "\n",
    "    def get_training_samples(self):\n",
    "        \"\"\"Generate the training samples for the given interventions.\"\"\"\n",
    "        if not self.training_samples:\n",
    "            self._create_intervened_samples()\n",
    "            self._update()\n",
    "        return self.training_samples\n",
    "    \n",
    "    def _create_intervened_samples(self):\n",
    "        \"\"\"Generate copies of base sample for each intervention and set proxies.\"\"\"\n",
    "        self.training_samples = []\n",
    "        for proxy, functions in self.interventions.items():\n",
    "            for func, parameters in functions.items():\n",
    "                if not isinstance(parameters, list):\n",
    "                    parameters = [parameters]\n",
    "                for params in parameters:\n",
    "                    sample = copy.deepcopy(self.base_sample)\n",
    "                    sample[proxy] = self.known_functions[func](self, *params)\n",
    "                    self.training_samples.append(sample)\n",
    "    \n",
    "    def _update(self):\n",
    "        \"\"\"Update the variables downstream of the proxies.\"\"\"\n",
    "        downstream = list(set(sum([self.intervened_graph.descendents(proxy) for proxy in self.proxies], [])))\n",
    "        need_update = list(set(downstream).difference(set(self.target)))\n",
    "        fixed = set(self.intervened_graph.vertices()).difference(downstream)\n",
    "\n",
    "        while need_update:\n",
    "            found_one = False\n",
    "            for update in need_update:\n",
    "                if set(self.intervened_graph.parents(update)) <= set(fixed):\n",
    "                    # Found one that can be updated\n",
    "                    found_one = True\n",
    "                    # Update this variable in all samples\n",
    "                    for sample in self.training_samples:\n",
    "                        argument = Variable(combine_variables(self.intervened_graph.parents(update), sample))\n",
    "                        sample[update] = learned[update](argument).data\n",
    "                    # Remove the updated one from the list\n",
    "                    need_update.remove(update)\n",
    "            assert found_one, \"Could not update any downstream variables {} from {}\".format(need_update, fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our self made format to specify interventions.\n",
    "# In a dict, for each proxy variable, we store another dict, which we call `functions`.\n",
    "# In `functions`, keys are preset strings that correspond to the `known_functions` in the `Intervention` class.\n",
    "# Current options: 'randn', 'rand', 'const', 'range'\n",
    "# Every value of `functions` must be a list of tuples (!),\n",
    "# where the tuples hold one or multiple scalar arguments (depending on the key).\n",
    "# Example:\n",
    "# intervention_spec = {\n",
    "#     'P': {'randn': [(0, 3), (0, 3)],\n",
    "#           'const': [(1,), (0,)],\n",
    "#           'range': [(-1, 1)]\n",
    "#          },\n",
    "#     'X': {'randn': [(0, 1), (0, 1), (0, 1)]\n",
    "#          },\n",
    "#     }\n",
    "\n",
    "intervention_spec = {\n",
    "    'P': {\n",
    "#          'randn': [(0, 3), (0, 3)],\n",
    "         'const':[(1,), (0,)]\n",
    "         },\n",
    "    }\n",
    "interventions = Interventions(graph, sample, intervention_spec)\n",
    "print(\"Sample size: {}, Number of interventions {}\".format(interventions.n_samples, interventions.n_interventions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct the _real_ SEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need to create/manage intervened samples differently:**\n",
    "\n",
    "* Can't have `intervention_values`, because just two different random samples should also be possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_and_freeze(model):\n",
    "    \"\"\"Copy a learned model and partially freeze parameters.\"\"\"\n",
    "    # Copy the original model for the target variable\n",
    "    corrected = copy.deepcopy(model)\n",
    "\n",
    "    # First freeze all parameters\n",
    "    for param in corrected.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Then only give gradients to the part that should be retrained for correction\n",
    "    # FIXME: the layer indices are hard coded. I have to find those out\n",
    "    # FIXME: not sure whether to finetune only weights or also biases?\n",
    "\n",
    "    # fine tune weights and bias:\n",
    "    for param in corrected.layers[0][0].parameters():\n",
    "        param.requires_grad = True\n",
    "    # fine tune only weights\n",
    "#     corrected.layers[0][0].weight.requires_grad = True\n",
    "\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_corrected(learned, interventions, batchsize=32, epochs=50):\n",
    "    # Some basic input checks\n",
    "    target = interventions.target\n",
    "    proxies = interventions.proxies\n",
    "    print(\"Correct for the effect of {} on {}.\".format(proxies, target))\n",
    "\n",
    "    print(\"Generate intervened samples...\", end=' ')\n",
    "    train_samples = interventions.get_training_samples()\n",
    "    print(\"DONE\")\n",
    "\n",
    "    # Sanity check\n",
    "    assert len(train_samples) == interventions.n_interventions, \"Number of interventions {} does not match number of training samples {}\".format(interventions.n_interventions, train_samples)\n",
    "    print(\"There is a total of {} interventions.\".format(len(train_samples)))    \n",
    "    \n",
    "    print(\"Freeze everything except first weights from {} to {}...\".format(proxies, target), end=' ')\n",
    "    corrected = copy_and_freeze(learned[target])\n",
    "    print(\"DONE\")\n",
    "\n",
    "    print(\"Set up the optimizer...\", end=' ')\n",
    "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, corrected.parameters()))\n",
    "    print(\"DONE\")\n",
    "    \n",
    "    print(\"Partially retrain the target model for correction...\", end=' ')\n",
    "    n_samples = interventions.n_samples\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        p = torch.randperm(n_samples).long()\n",
    "                    \n",
    "        for i1 in range(0, n_samples, batchsize):\n",
    "            # sample data\n",
    "            i2 = min(i1 + batchsize, n_samples)\n",
    "\n",
    "            # reset gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            Ys = Variable(torch.zeros(batchsize, interventions.n_interventions))\n",
    "            for i, sample in enumerate(train_samples):\n",
    "                argument = Variable(combine_variables(interventions.intervened_graph.parents(target), sample)[i1:i2, :])\n",
    "                Ys[:, i] = corrected(argument).squeeze()\n",
    "\n",
    "            loss = torch.sum(torch.var(Ys, dim=1))\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # parameter update\n",
    "            opt.step()\n",
    "    print(\"DONE\")\n",
    "    print(\"Finished correction.\")\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalizer = train_corrected(learned, interventions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mathbb{R} \\to \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_linear(support=[0,1], slope=1, constant=0, n=1024, eps=0.1):\n",
    "    \"\"\"Simple linear data with noise.\"\"\"\n",
    "    x = torch.rand(n, 1) * (support[1] - support[0]) + support[0]\n",
    "    y = slope * x + constant + eps * torch.rand(n, 1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_quadratic(support=[0,1], a=1, b=0, c=0, n=1024, eps=0.1):\n",
    "    \"\"\"Simple linear data with noise.\"\"\"\n",
    "    x = torch.rand(n, 1) * (support[1] - support[0]) + support[0]\n",
    "    y = a * x**2 + b * x + c + eps * torch.rand(n, 1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = example_quadratic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.numpy(), y.numpy(), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = train(MLP([1, 128, 1]), x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_x = torch.linspace(-10, 10, steps=1024)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(plt_x.numpy(), pred(Variable(plt_x)).data.numpy(), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mathbb{R}^2 \\to \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 2) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[:, 0] * 2 - 1.5 * x[:, 1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.numpy()[:, 0], y.numpy(), '.')\n",
    "plt.plot(x.numpy()[:, 1], y.numpy(), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train(MLP([2, 128, 1]), x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_x = torch.randn(1000, 2) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(plt_x.numpy()[:, 0], test(Variable(plt_x)).data.numpy(), '.')\n",
    "plt.plot(plt_x.numpy()[:, 1], test(Variable(plt_x)).data.numpy(), '.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
